{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0189e877",
   "metadata": {},
   "source": [
    "# GPT1 Pretrain model \n",
    "\n",
    "\n",
    "\n",
    "## Transformer model , GPT model ( Pretrain ) 비교\n",
    "\n",
    "- GPT pretrain 의 경우 인코더가 없다\n",
    "- 디코더에서도 마스크드 멀티 셀프어텐션 부분만을 사용하고 인코더 디코더 멀티헤드 어텐션층은 없다\n",
    "- 인코더와 인코더-디코더 멀티헤드 어텐션이 없으므로 모델 인풋이 디코더 인풋이 되며 이로 인해 인풋이 달라진다.\n",
    "- 인풋에서 포지셔널 인코딩 대신 포지셔널 임베딩을 사용하였다.\n",
    "- GPT pretrain의 경우 비지도 학습이므로 데이터셋의 구성이 달라진다 target을 따로 준비하지 않고 문장의 다음 토큰이 타겟이 됨\n",
    "\n",
    "\n",
    "\n",
    "## 기존 transformer 코드에서 변경사항\n",
    "- 인코더, 인코더 레이어 제거\n",
    "- PositionalEncoding 클래스 -> PositionalEmbedding 클래스\n",
    "- 디코더 레이어 변경\n",
    "    - 인코더의 출력을 디코더 레이어에 입력으로 사용하는 부분 제거 ( enc_output 과 padding_mask 제거 )\n",
    "    - 인코더-디코더 Multihead Attention 제거( 레이어 제거 -> 전차 연결 수정 )\n",
    "- 디코더 변경\n",
    "    - 인코더의 출력을 디코더 레이어에 입력으로 사용하는 부분 제거 (enc_outputs, padding_mask 제거)\n",
    "    - (토큰 임베딩 -> 포지셔널 인코딩) => (토큰 임베딩 + 포지셔널 임베딩)\n",
    "- transformer 모델 구현을 GPT pretrain 모델로 변경\n",
    "    - 인코더 디코더 패딩 마스크 제거 ( enc_padding_mask, dec_padding_mask)\n",
    "    - dec_inputs 제거하고 inputs 로 입력 단일화\n",
    "    - 인코더 실행 부분 제거\n",
    "- 예측 문장 생성 함수 변경\n",
    "    - pretrain 모델에 맞게 현재 받은 입력을 바탕으로 다음 토큰을 예측해 문장을 완성하도록 변경\n",
    "    - END 토큰을 예측 못할때 반복되는 경우가 많아 반복 제거 코드 추가\n",
    "- 훈련용 데이터셋 구성 변경 \n",
    "    - 기존 input, dec_input, output에서 input output으로 변경\n",
    "    - input 과 output 모두 입력 시퀀스로 input은 처음부터 마지막 바로전까지, output은 두번째부터 마지막까지로 변경\n",
    "- 모델 layer 수 12개로 변경하려하였다가 데이터가 적을때는 오히려 안좋다고하여 다시 2로 변경\n",
    "    \n",
    " *** 변경 코드에 GPT_QUEST prefix로 주석을 모두 달았으므로 참고 ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c66a9de",
   "metadata": {},
   "source": [
    "## Import & Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95b92a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "chatbot_data_file = \"~/aiffel/transformer_chatbot/data/ChatbotData.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c66133",
   "metadata": {},
   "source": [
    "## 클래스 및 함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dd065d",
   "metadata": {},
   "source": [
    "### 변경하지 않은 함수 & 제거 함수\n",
    "- 제거 함수\n",
    "    - encoder_layer 함수\n",
    "    - encoder 함수\n",
    "- 변경하지 않은 함수 ( 생략 아래 참조 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdd27282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # 최대 길이 MAX_LENGTH 이하인 경우에만 데이터셋으로 허용\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # 최대 길이 MAX_LENGTH로 모든 데이터셋을 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "# 패딩 마스크\n",
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "# 룩어헤드 마스크\n",
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)\n",
    "\n",
    "\n",
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 가중치를 정규화\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 패딩에 마스크 추가\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)    # 매우 작은 값\n",
    "\n",
    "  # softmax적용\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "  return output\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # Q, K, V에 각각 Dense를 적용합니다\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # 스케일드 닷 프로덕트 어텐션 함수\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "#모델 훈련용\n",
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "def sentence_generation(sentence):\n",
    "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "  prediction = decoder_inference(sentence)\n",
    "\n",
    "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('입력 : {}'.format(sentence))\n",
    "  print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence\n",
    "\n",
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence):\n",
    "  # 입력받은 sentence를 소문자로 변경하고 양쪽 공백을 제거\n",
    "  sentence = sentence.lower().strip() # 영문이 포함된 문장이 있긴해서 포함\n",
    "\n",
    "  # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "  # 한글 영문 그리고 일부 문자를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
    "  sentence = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z.?!,]+\", \" \",sentence) \n",
    "  sentence = sentence.strip()\n",
    "  return sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7a0162",
   "metadata": {},
   "source": [
    "### 변경된 코드\n",
    " *  변경 부분 GPT_QUEST preifx 로 주석 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a5540ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#GPT_QUEST PositionalEncoding 클래스 -> PositionalEmbedding 클래스\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_position, d_model):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # 포지셔널 임베딩 레이어 생성\n",
    "        self.position_embedding = tf.keras.layers.Embedding(input_dim=max_position, output_dim=d_model)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        seq_length = tf.shape(inputs)[1]\n",
    "\n",
    "        # (batch_size, seq_length) 위치 인덱스 생성\n",
    "        position_indices = tf.tile(\n",
    "            tf.expand_dims(tf.range(seq_length), 0), [batch_size, 1]\n",
    "        )\n",
    "\n",
    "        # 위치 임베딩 조회: (batch_size, seq_length, d_model)\n",
    "        position_embeddings = self.position_embedding(position_indices)\n",
    "\n",
    "        return inputs + position_embeddings\n",
    "    \n",
    "    \n",
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    \n",
    "  # GPT_QUEST - enc_outputs, padding_mask 초기화 제거 \n",
    "\n",
    "  # 첫 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "  #  마스크드 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "    \n",
    "  # GPT_QUEST 인코더-디코더 어텐션 제거\n",
    "\n",
    "  # 두 번째 서브 레이어 : 2개의 완전연결층 (피드포워드)\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention1)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention1)       # GPT_QUEST 전차 연결 1번째 레이어 출력으로 변경\n",
    "                                                \n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, look_ahead_mask],         # GPT_QUEST 불필요한 enc_outputs, padding_mask 제거\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "\n",
    "\n",
    "# 디코더 함수로 구현\n",
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  look_ahead_mask = tf.keras.Input(shape=(1, None, None), name='look_ahead_mask')\n",
    "  # GPT_QUEST 불필요한 enc_outputs, padding_mask 제거\n",
    "\n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  #GPT_QUEST 포지셔널 인코딩 => 포지셔널 임베딩\n",
    "  embeddings = PositionalEmbedding(MAX_LENGTH, d_model)(embeddings)\n",
    "\n",
    "  # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, look_ahead_mask])      # GPT_QUEST 불필요한 enc_output, padding_mask 제거\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, look_ahead_mask],       # GPT_QUEST 불필요한 enc_output, padding_mask 제거\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "\n",
    "# GPT_QUEST 트랜스포머 모델 -> GPT pretrain 모델로 변경\n",
    "def GPT_pretrain(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"GPT_pretrain\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  # GPT_QUEST dec_inputs 제거\n",
    "  #   dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "\n",
    "  # GPT_QUEST 인코더 디코더 패딩 마스크 제거 ( enc_padding_mask, dec_padding_mask)\n",
    "\n",
    "  # 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(inputs)\n",
    "\n",
    "  # GPT_QUEST -  인코더 실행 제거\n",
    "\n",
    "  # 디코더\n",
    "  dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[inputs, look_ahead_mask])\n",
    "\n",
    "  # 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=outputs, name=name)        # GPT_QUEST dec_input 제거\n",
    "\n",
    "\n",
    "# 문장 예측 출력용 \n",
    "def decoder_inference(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # GPT_QUEST - pretrain 모델은 QnA가 아닌 다음 토큰을 예측하는 것이므로 문장 예측 수정\n",
    "  # 입력된 문장을 토크나이즈하고 START_TOKEN 만을 붙여 이 후 예측 토큰을 확인하도록 변경 \n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence), axis=0)\n",
    "\n",
    "  output_sequence = sentence\n",
    "  \n",
    "  past_tokens = set()  # GPT_QUEST - 반복 제거용\n",
    "\n",
    "  # 디코더의 인퍼런스 단계\n",
    "  for i in range(MAX_LENGTH - tf.shape(sentence)[1]):\n",
    "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "    predictions = model(inputs=output_sequence, training=False)      # GPT_QUEST output sequece 를 입력으로 변경\n",
    "    predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # 현재 예측한 단어의 정수\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "    \n",
    "    # GPT_QUEST 반복 방지: 동일 토큰이 여러 번 나오면 중단 (옵션)\n",
    "    if int(predicted_id[0][0].numpy()) in past_tokens:\n",
    "        break\n",
    "    past_tokens.add(int(predicted_id[0][0].numpy()))\n",
    "    \n",
    "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output_sequence, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e64eee",
   "metadata": {},
   "source": [
    "## 데이터 준비\n",
    " songys/Chatbot_data\n",
    " https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07e6cb3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ori = pd.read_csv(chatbot_data_file, encoding='utf-8')\n",
    "\n",
    "data_ori.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd7be7",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "\n",
    "- QnA를 합쳐 처리해야하지만 이전 처리 과정들을 스킵하고자 데이터셋 준비 부분에서 Q 와 A를 합쳐서 데이터 구성하도록 변경하였습니다.\n",
    "- 따라서 이전 코드들은 그대로 사용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639020ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q        0\n",
      "A        0\n",
      "label    0\n",
      "dtype: int64\n",
      "questions 수: 11823\n",
      "answers 수: 11823\n",
      "총 단어수: 20528\n",
      "\n",
      "랜덤 Q&A 샘플 3개:\n",
      "\n",
      "Q(551): 나 좀 안 건들였으면 좋겠어\n",
      "A(551): 많이 지쳤나봐요 .\n",
      "\n",
      "Q(3727): 이상한 소문이 돌아\n",
      "A(3727): 소문은 소문일 뿐이에요 .\n",
      "\n",
      "Q(10231): 썸 타는데 사랑하다고 말할 수 있음 ?\n",
      "A(10231): 좋아함을 점프했네요 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#전처리 \n",
    "\n",
    "# 결측 데이터 체크\n",
    "print(data_ori.isnull().sum())\n",
    "\n",
    "# 데이터를 로드하고 전처리하여 질문을 questions, 답변을 answers에 저장합니다.\n",
    "questions = [preprocess_sentence(sentence) for sentence in data_ori['Q']]\n",
    "answers = [preprocess_sentence(sentence) for sentence in data_ori['A']]\n",
    "\n",
    "print('questions 수:', len(questions))\n",
    "print('answers 수:', len(answers))\n",
    "\n",
    "word_cnt = len({word for s in questions + answers for word in s.split()})\n",
    "print(\"총 단어수:\", word_cnt)\n",
    "\n",
    "indices = random.sample(range(len(questions)), 3)\n",
    "\n",
    "print('\\n랜덤 Q&A 샘플 3개:\\n')\n",
    "for i in indices:\n",
    "    print(f\"Q({i}): {questions[i]}\")\n",
    "    print(f\"A({i}): {answers[i]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aa89246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
    "\n",
    "# target_vocab_size 값을 조정하며 테스트 \n",
    "# target_vocab_size를 작게하는 경우 단어를 여러개의 서브워드로 분리 답변이 제대로 출력되지 못하는 경우 발생\n",
    "# taget_vovab_size를 크게 하는 경우에는 답변이 더욱 엉뚱해졌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45ed71bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8141]\n",
      "END_TOKEN의 번호 : [8142]\n",
      "8143\n"
     ]
    }
   ],
   "source": [
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98c836c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 551번째 질문 샘플: [27, 24, 20, 622, 266, 5030, 299]\n",
      "정수 인코딩 후의 551번째 답변 샘플: [19, 1491, 1]\n",
      "\n",
      "정수 인코딩 후의 3727번째 질문 샘플: [1395, 5178, 1040]\n",
      "정수 인코딩 후의 3727번째 답변 샘플: [2585, 33, 2585, 85, 543, 1]\n",
      "\n",
      "정수 인코딩 후의 10231번째 질문 샘플: [61, 2185, 1100, 313, 1709, 4, 2494, 2]\n",
      "정수 인코딩 후의 10231번째 답변 샘플: [140, 2417, 1624, 2177, 3197, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 각 토큰을 고유한 정수로 변환\n",
    "for i in indices:\n",
    "    print(f'정수 인코딩 후의 {i}번째 질문 샘플: {tokenizer.encode(questions[i])}')\n",
    "    print(f'정수 인코딩 후의 {i}번째 답변 샘플: {tokenizer.encode(answers[i])}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a76f87ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 질문 길이: 16\n",
      "최대 답변 길이: 24\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# 각 질문과 답변의 단어 수 계산\n",
    "questions_lengths = [len(sentence.split()) for sentence in questions]\n",
    "answers_lengths = [len(sentence.split()) for sentence in answers]\n",
    "\n",
    "# 최대 길이 찾기\n",
    "max_question_length = max(questions_lengths)\n",
    "max_answer_length = max(answers_lengths)\n",
    "\n",
    "print(f\"최대 질문 길이: {max_question_length}\")\n",
    "print(f\"최대 답변 길이: {max_answer_length}\")\n",
    "\n",
    "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
    "MAX_LENGTH = 32\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb8e81af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8143\n",
      "필터링 후의 질문 샘플 개수: 11823\n",
      "필터링 후의 답변 샘플 개수: 11823\n"
     ]
    }
   ],
   "source": [
    "# 토크나이징 후 MAX_LENGTH 초과 샘플 제거\n",
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6d11e5",
   "metadata": {},
   "source": [
    "### GPT pretrain 관련 훈련 데이터셋 변경\n",
    "- GPT_QUEST pretrain 모델에 사용하기 위해 Q A 데이터를 하나의 문장 데이터로 합침\n",
    "    - inputs 는 문장의 처음부터 마지막 이전 토큰까지\n",
    "    - outputs 는 문장의 두번째부터 마지막 토큰까지로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d46a46c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT_QUEST  Q와 A를 순차적으로 하나씩 sentences에 담기\n",
    "sentences = np.array([item for pair in zip(questions, answers) for item in pair])\n",
    "\n",
    "\n",
    "# 데이터를 섞어서 batch size로 분리\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': sentences[:, :-1]\n",
    "        # GPT_QUEST dec_inputs 제거하고 inputs로 대체\n",
    "    },\n",
    "    {\n",
    "        'outputs': sentences[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e819243",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8b1bbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"GPT_pretrain\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    3147008     inputs[0][0]                     \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8143)   2092751     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,239,759\n",
      "Trainable params: 5,239,759\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 #  디코더층의 개수\n",
    "D_MODEL = 256 # 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = GPT_pretrain(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbcdd126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 컴파일 \n",
    "\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93006875",
   "metadata": {},
   "source": [
    "## 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39b71779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "370/370 [==============================] - 14s 30ms/step - loss: 1.7022 - accuracy: 0.0288\n",
      "Epoch 2/10\n",
      "370/370 [==============================] - 11s 30ms/step - loss: 1.3526 - accuracy: 0.0477\n",
      "Epoch 3/10\n",
      "370/370 [==============================] - 12s 31ms/step - loss: 1.2225 - accuracy: 0.0548\n",
      "Epoch 4/10\n",
      "370/370 [==============================] - 12s 32ms/step - loss: 1.1122 - accuracy: 0.0612\n",
      "Epoch 5/10\n",
      "370/370 [==============================] - 12s 32ms/step - loss: 1.0040 - accuracy: 0.0695\n",
      "Epoch 6/10\n",
      "370/370 [==============================] - 12s 31ms/step - loss: 0.8966 - accuracy: 0.0779\n",
      "Epoch 7/10\n",
      "370/370 [==============================] - 11s 31ms/step - loss: 0.7986 - accuracy: 0.0863\n",
      "Epoch 8/10\n",
      "370/370 [==============================] - 11s 31ms/step - loss: 0.7118 - accuracy: 0.0956\n",
      "Epoch 9/10\n",
      "370/370 [==============================] - 11s 31ms/step - loss: 0.6417 - accuracy: 0.1043\n",
      "Epoch 10/10\n",
      "370/370 [==============================] - 11s 31ms/step - loss: 0.5896 - accuracy: 0.1119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7c8b3887f160>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca95d550",
   "metadata": {},
   "source": [
    "## 모델 평가\n",
    "\n",
    "- 완성된 문장을 쓰게되면 다음 토큰을 END로 예측해 문장이 끝나고 해당 문장을 그대로 반환한다.\n",
    "- 의미상 맞지 않는 문장을 구사하는 경우가 있지만 어느정도 문장을 완성해주긴 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24bb943c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : ㅠㅠ\n",
      "출력 : ㅠㅠ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ㅠㅠ'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('ㅠㅠ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f7fbd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 오늘 뭐했어?\n",
      "출력 : 오늘 뭐했어 ?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'오늘 뭐했어 ?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('오늘 뭐했어?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0903d0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 드라마가\n",
      "출력 : 드라마가의 연애에요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'드라마가의 연애에요 .'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('드라마가')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "038a8c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 영화를\n",
      "출력 : 영화를가 재미 없어\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'영화를가 재미 없어'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('영화를')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9af6be24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 뭐\n",
      "출력 : 뭐부터 바꿔야 할까\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'뭐부터 바꿔야 할까'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('뭐')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbe68d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 음악 \n",
      "출력 : 음악을 들어보세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'음악을 들어보세요 .'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('음악 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e78e3a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 점심 먹고\n",
      "출력 : 점심 먹고는 살아야죠 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'점심 먹고는 살아야죠 .'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('점심 먹고')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d16e1fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 너무 \n",
      "출력 : 너무힘들어\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'너무힘들어'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('너무 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a59905fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 오늘은 그것을\n",
      "출력 : 오늘은 그것을때는 솔직하게 이야기하고 풀어보세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'오늘은 그것을때는 솔직하게 이야기하고 풀어보세요 .'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('오늘은 그것을')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a44605",
   "metadata": {},
   "source": [
    "## 회고\n",
    "\n",
    "- 현재 사용되는 GPT의 초기 모델에 대해 알수 있어 좋았습니다.\n",
    "- 출력이 제대로 되지 않아 한참 고생했는데 완벽하게 해결은 못한거 같지만 그래도 이것저것 알아보면서 공부가 되었습니다.\n",
    " 아직도 파이썬 데이터 구조가 익숙해지지 않은듯하여 더 파이썬을 다뤄봐야겠다고 생각이 들었습니다.\n",
    "- 특정 토큰이 반복되는 현상이나 깨진 문자 안맞는 단어등이 생성되는데 어떻게 하면 더 잘 처리할 수 있을까 코드상의 문제는 아닐까 생각이 많지만 더 해볼 시간이 없는게 아쉽습니다.\n",
    "- 실제 GPT pretrain의 경우는 START END 토큰 없이 text 덩어리를 학습한다고 하는데 일단 샘플 데이터가 이어지는 문장들도 아니고해서 START와 END로 문장을 분리해서 학습하고 문장 예측을 하였습니다. START END 토큰 없이 하면 어떤 결과가 나올까 궁금한데 다음에 시간이 나면 해봐야겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ace9ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
